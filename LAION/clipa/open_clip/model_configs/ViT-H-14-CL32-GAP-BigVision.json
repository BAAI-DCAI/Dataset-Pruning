{
    "embed_dim": 1024,
    "vision_cfg": {
        "image_size": 224,
        "layers": 32,
        "width": 1280,
        "head_width": 80,
        "patch_size": 14,
        "gelu_approximate": "tanh",
        "ln_pre": false,
        "pool_style": "big_vision_gap",
        "global_average_pool": true
    },
    "text_cfg": {
        "context_length": 32,
        "vocab_size": 32000,
        "bert_tokenizer": true,
        "vocab_path": "gs://vit_models/lit/bert/uncased_L-12_H-768_A-12/vocab.txt",
        "width": 1024,
        "heads": 16,
        "layers": 24,
        "gelu_approximate": "tanh",
        "pool_style": "big_vision_last",
        "attention_mask": false
    }
}